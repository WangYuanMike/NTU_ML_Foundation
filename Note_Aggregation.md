# Aggregation (Ensemble)

## Table of Contents
[Uniform Blending](#uniform-blending)  
[Linear Blending](#linear-blending)  
[Bagging](#bagging)  
[Adaptive Boosting](#adaptive-boosting)  
[Decision Tree](#decision-tree)  
[Random Forest](#random-forest)  
[AdaBosst Decision Tree](#adaboost-decision-tree)  
[Gradient Boosted Decision Tree](#gradient-boosted-decision-tree)  
[Summary](#summary)

## [Uniform Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend many possibly weaker hypotheses **g_t()** to get one strong and moderate hypothesis **G()** 
### Why
- strong -> feature transform -> cure underfitting
- moderate -> regularization -> cure overfitting
- **Requires diverse g_t()** -> diversity + democracy -> majority can correct minority
- **Bias-Variance analysis**
    - Eout(G) or Eout(g_t) can be both decoupled into bias and variance
    - Eout(G) and Eout(g_t) has the same bias
    - And var(G) = var(g_t) / n (n is the number of weaker hypotheses for aggregation)
    - That's why Eout(G) is **more stable** than Eout(g_t)  
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/bias_variance_0.png) .   
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/bias_variance_1.png) .  
### How
- Classification: vote G() by many g_t(), each with 1 ballot .   
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/uniform_blending_classification.png)
- Regression: take average of all g_t() .   
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/uniform_blending_regression.png)
### When and Where
- Nearly no practical use case
- But it is the foundation for advanced aggregation model and theoretical analysis
### Cons
- "1 ballot each" may not be able to give the optimal solution
- Uniform blending is a special case of linear blending, check more details in the next section

## [Linear Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend the weak hypotheses with different "ballots"  
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/linear_blending.png)
### Why
- Linear blending could provide more combination to approximate the target
- g_t() can be deemed as a feature transformation on x, alpha is corresponding to weight
- constraint on alpha is not important, since negative g_t() can be patched with a negative alpha
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/linear_blending_analogue.png)
### How
- use D_train to train many g_t-()
- transform data in D_val into phi-(x) with these g_t-()
- select best alpha with the phi-(x)
- retrain the weak hypotheses with D_train + D_val to get corresponding g_t()
- return G() = linear_model(alpha, g_t())
### When and Where
- This [paper](https://www.csie.ntu.edu.tw/~htlin/paper/doc/wskdd11cup_one.pdf) describes an example of using linear blending and any blending to win the 2011 KDDCup competition (music rating prediction)
### Cons
- **Any Blending(Stacking)** means one can also use non-linear model (e.g. neural networks) for blending
- Diverse is important, e.g. parameters, hypotheses, alpha, and so on. But in blending, data is not diverse. In next section, Bagging would provide some randomness in training data.

## [Bagging](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Bootstrap means re-sample N examples from D uniformly with replacement
- Bagging means **Bootstrap Aggregation**, i.e. do uniform blending on data set generated by bootstrap 
### why
- Bagging is to approximate the theoretical consensus g_bar() through bootstrapping, i.e. a way of simulating randomness with finite data set
### How
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/bagging_algorithm.png)
### When and Where
- Bagging should perform reasonably well when base model is sensitive to data randomness
### Cons
- Stable classifiers (e.g. KNN) will not benefit from bagging
- Bagging only aggregates hypotheses uniformly. Next section will introduce how AdaBoost mitigates this cons

## [Adaptive Boosting](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/208_handout.pdf)
### What
- AdaBoost(Adaptive Boosting) combines weak hypotheses linearly
- It simulates bootstrapping by computing example-weighted error
- It adjusts example weight according to error of last iteration
- It learns alpha for aggregation during learning weak hypotheses
- The basic version of AdaBoost uses decision stump as weak hypotheses, and the whole model is called **AdaBoost-Stump**
### Why
- Example-weighted error keeps the data randomness in bagging
- Example weight is adjusted to learn a hypothesis which is very different from the previous one
- Alpha is adjusted according to example-weighted error so that good hypothesis would get higher alpha
- In summary, the main character of AdaBoost is to direct the model to focus on the key examples by scaling up the incorrect ones and scaling down the correct ones
### How
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/adaboost_algorithm.png)
### When and Where
- AdaBoost is a very practical and efficient aggregation model which is used widely
### Cons
- It is still a linear aggregation model, and every weak hypothesis is learnt with the whole set of learning data
- In contrast, decision tree is a conditional aggregation model and learns weak hypotheses by focusing on a subset of learning data. See more detail in next section

## [Decision Tree](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/209_handout.pdf)
### What
- A traditional model that realizes conditional aggregation
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/blending_aggregation_learning.png)
### Why
- Mimic human's decision making process
- Good explainability
- Easy to implement (recursive tree model)
- Efficient in prediction and training
### How
- C&RT algorithm (Classification & Regression Tree)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_algorithm.png)
- Impurity function
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_impurity_function.png)
- Termination condition
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_termination_condition.png)
- Regularization by pruning
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_pruning.png)
- Branching on categorical feature
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_categorical_feature.png)
- Missing feature by surrogate branching
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/decision_tree_missing_feature.png)
### When and Where
- Decision tree is a practical model
- More importantly it is a core building block for **Random Forest** and **Gradient Boosting**
### Cons
- Heuristics with mostly little theoretical explanations
- Easy to overfit(fully grown tree usually has large variance) or underfit(not able to find a rule to split data properly)
- Solution is **Random Forest** or **Gradient Boosting** introduced in the next sections

## [Random Forest](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/210_handout.pdf)
### What
- Random forest == bagging + fully-grown C&RT decision tree on randomly projected subspace
### Why
- Decision tree (esp. fully-grown tree) has large variance, and bagging is able to reduce variance
- Inherit pros of C&RT
- Highly parallel/efficient to learn
### How
- bagging + fully-grown C&RT decision tree
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_algorithm.png)
- Diversify by feature selection (feature expansion) when computing decision condition each time
    - feature selection: select some features from original ones (a special case of feature expansion)
    - feature expansion: use some random linear combination of original feature basis to form a projection matrix, and use it project original features onto random subspace
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_sample_feature.png)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_random_feature.png)
- Use OOB (out-of-bag) examples to select G() (similar as validation)
    - about 1/e of examples are out-of-bag
    - no re-train needed, compared with validation
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_oob_definition.png)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_oob_validation.png)
- **Feature selection by importance**
    - In general, feature importance can be computed with feature permutation, i.e. if a random feature value does impact the classification/regression result, then it is an importance feature in this model
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_feature_importance.png)
    - In random forest, the feature permutation is only performed within OOB examples
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/rf_feature_importance_permutation_test.png)
### When and Where
- Random forest is also a very popular model 
### Cons
- May need lots of trees if the whole random process is too unstable

## [AdaBoost Decision Tree](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)
### What
- Use decision tree as weak hypotheses in AdaBoost
### Why
- Random forest just combines the weak hypotheses uniformly, while AdaBoost decision tree combines them linearly
- This is a step to extend the model from binary classification to regression (check more details in next section **Gradient Boosting Decision Tree**)
### How
- Use bootstrap to sample examples according to weight u, to avoid changing decision tree algorithm due to requirement of AdaBoost
- Restrict the height of decision tree to avoid a fully-grown decision tree (fully-grown tree -> E_in_u = 0 -> epsilon_t = 0 -> alpha_t = infinite)
- When height is restricted to one, the AdaBoost Decision Tree model becomes AdaBoost-Stump
### When and Where
- AdaBoost Decision Tree is a special case of Gradient Boost Decision Tree, which is a step to understand Boosting model from Gradient Descent perspective
### Gradient Descent for AdaBoost
- Goal of AdaBoost is to reduce u_t (large margin decision boundary -> ys needs to be large -> u_t needs to be small -> use exponential error as error measure for AdaBoost)
TOD: add image
- Gradient Descent requires to find two parameters to reduce error:
    - the steepest unit vector **v** to reduce the error => In AdaBoost, vector v is analogous to function h(x)
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/adaboost_dtree_h(x)_0.png)
    - the step length **eta** to go => In AdaBoost, eta is analogous to alpha
    ![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/adaboost_dtree_h(x)_1.png)

## [Gradient Boosted Decision Tree](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)
### What
- A generic boosting model using decision tree as weak hypotheses
### Why
- Extend the boosting model to regression problems
### How
- Use residual (y - s_t) to find (1) the function h(x) (usually) and (2) the alpha for gradient descent
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/GBDT_algorithm.png)
### When and Where
- It is a very mature and popular model
### Cons
- Gradient Boosting builds trees one by one, therefore it is more efficient to train Random Forest than Gradient Boosting

## [Summary](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/summary_blending.png)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/summary_aggregation_learning.png)
![alt_text](https://github.com/WangYuanMike/NTU_ML_Foundation/blob/master/Aggregation/summary_aggregation_of_aggregation.png)
