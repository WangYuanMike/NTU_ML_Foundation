# Aggregation (Ensemble)

## Table of Contents
[Uniform Blending](#uniform-blending)
[Linear Blending](#linear-blending)
[Bagging](#bagging)
[Adaptive Boosting](#adaptive-boosting)  
[Decision Tree](#decision-tree)  
[Random Forest](#random-forest)  
[Gradient Boosted Decision Tree](#gradient-boosted-decision-tree)  
[Summary](#summary)

## [Uniform Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend many possibly weaker hypotheses **g_t()** to get one strong and moderate hypothesis **G()** 
### Why
- strong -> feature transform -> cure underfitting
- moderate -> regularization -> cure overfitting
- **Requires diverse g_t()** -> diversity + democracy -> majority can correct minority
- Bias-Variance analysis
    - Eout(G) or Eout(g_t) can be both decoupled into bias and variance
    - Eout(G) and Eout(g_t) has the same bias
    - And var(G) = var(g_t) / n (n is the number of weaker hypotheses for aggregation)
    - That's why Eout(G) is **more stable** than Eout(g_t)  
    TODO: ADD IMAGE
### How
- Classification: vote G() by many g_t(), each with 1 ballot
TODO: ADD IMAGE
- Regression: take average of all g_t()
TODO: ADD IMAGE
### When and Where
- Nearly no practical use case
- But it is the foundation for advanced aggregation model and theoretical analysis
### Cons
- "1 ballot each" may not be able to give the optimal solution
- Uniform blending is a special case of linear blending, check more details in the next section

## [Linear Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend the weak hypotheses with different "ballots"  
TODO: ADD IMAGE
### Why
- Linear blending could provide more combination to approximate the target
- g_t() can be deemed as a feature transformation on x, alpha is corresponding to weight
- constraint on alpha is not important, since negative g_t() can be patched with a negative alpha
TODO: ADD IMAGE
### How
- use D_train to train many g_t-()
- transform data in D_val into phi-(x) with these g_t-()
- select best alpha with the phi-(x)
- retrain the weak hypotheses with D_train + D_val to get corresponding g_t()
- return G() = linear_model(alpha, g_t())
### When and Where
- This [paper](https://www.csie.ntu.edu.tw/~htlin/paper/doc/wskdd11cup_one.pdf) describes an example of using linear blending and any blending to win the 2011 KDDCup competition (music rating prediction)
### Cons
- **Any Blending(Stacking)** means one can also use non-linear model (e.g. neural networks) for blending
- Diverse is important, e.g. parameters, hypotheses, alpha, and so on. But in blending, data is not diverse. In next section, Bagging would provide some randomness in training data.

## [Bagging](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Bootstrap means re-sample N examples from D uniformly with replacement
- Bagging means Bootstrap Aggregation, i.e. do uniform blending on data set generated by bootstrap 
### why
- Bagging is to approximate the theoretical consensus g_bar() through bootstrapping, i.e. a way of simulating randomness with finite data set
### How
- Described in section "What" above
### When and Where
- Bagging should perform reasonably well when base model is sensitive to data randomness
### Cons
- Bagging only relies on data randomness to get diversity, while AdaBoost provides a well-defined algorithm to get more diverse hypotheses

## [Adaptive Boosting](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/208_handout.pdf)

## [Summary](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)

