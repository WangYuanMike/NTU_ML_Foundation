# Aggregation (Ensemble)

## Table of Contents
[Uniform Blending](#uniform-blending)  
[Linear Blending](#linear-blending)  
[Bagging](#bagging)  
[Adaptive Boosting](#adaptive-boosting)  
[Decision Tree](#decision-tree)  
[Random Forest](#random-forest)  
[Gradient Boosted Decision Tree](#gradient-boosted-decision-tree)  
[Summary](#summary)

## [Uniform Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend many possibly weaker hypotheses **g_t()** to get one strong and moderate hypothesis **G()** 
### Why
- strong -> feature transform -> cure underfitting
- moderate -> regularization -> cure overfitting
- **Requires diverse g_t()** -> diversity + democracy -> majority can correct minority
- Bias-Variance analysis
    - Eout(G) or Eout(g_t) can be both decoupled into bias and variance
    - Eout(G) and Eout(g_t) has the same bias
    - And var(G) = var(g_t) / n (n is the number of weaker hypotheses for aggregation)
    - That's why Eout(G) is **more stable** than Eout(g_t)  
    TODO: ADD IMAGE
### How
- Classification: vote G() by many g_t(), each with 1 ballot
TODO: ADD IMAGE
- Regression: take average of all g_t()
TODO: ADD IMAGE
### When and Where
- Nearly no practical use case
- But it is the foundation for advanced aggregation model and theoretical analysis
### Cons
- "1 ballot each" may not be able to give the optimal solution
- Uniform blending is a special case of linear blending, check more details in the next section

## [Linear Blending](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Blend the weak hypotheses with different "ballots"  
TODO: ADD IMAGE
### Why
- Linear blending could provide more combination to approximate the target
- g_t() can be deemed as a feature transformation on x, alpha is corresponding to weight
- constraint on alpha is not important, since negative g_t() can be patched with a negative alpha
TODO: ADD IMAGE
### How
- use D_train to train many g_t-()
- transform data in D_val into phi-(x) with these g_t-()
- select best alpha with the phi-(x)
- retrain the weak hypotheses with D_train + D_val to get corresponding g_t()
- return G() = linear_model(alpha, g_t())
### When and Where
- This [paper](https://www.csie.ntu.edu.tw/~htlin/paper/doc/wskdd11cup_one.pdf) describes an example of using linear blending and any blending to win the 2011 KDDCup competition (music rating prediction)
### Cons
- **Any Blending(Stacking)** means one can also use non-linear model (e.g. neural networks) for blending
- Diverse is important, e.g. parameters, hypotheses, alpha, and so on. But in blending, data is not diverse. In next section, Bagging would provide some randomness in training data.

## [Bagging](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/207_handout.pdf)
### What
- Bootstrap means re-sample N examples from D uniformly with replacement
- Bagging means **Bootstrap Aggregation**, i.e. do uniform blending on data set generated by bootstrap 
### why
- Bagging is to approximate the theoretical consensus g_bar() through bootstrapping, i.e. a way of simulating randomness with finite data set
### How
TODO: add image
### When and Where
- Bagging should perform reasonably well when base model is sensitive to data randomness
### Cons
- Stable classifiers (e.g. KNN) will not benefit from bagging
- Bagging only aggregates hypotheses uniformly. Next section will introduce how AdaBoost mitigates this cons

## [Adaptive Boosting](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/208_handout.pdf)
### What
- AdaBoost(Adaptive Boosting) simulates bootstrapping by computing example-weighted error
- adjusts example weight according to error of last iteration
- learns alpha for aggregation during learning weak hypotheses
### Why
- Example-weighted error keeps the data randomness in bagging
- Example weight is adjusted to learn a hypothesis which is very different from the previous one
- Alpha is adjusted according to example-weighted error so that good hypothesis would get higher alpha
- In summary, the main character of AdaBoost is to direct the model to focus on the key examples by scaling up the incorrect ones and scaling down the correct ones
### How
TODO: add image
### When and Where
- AdaBoost is a very practical and efficient aggregation model which is used widely
### Cons
- It is still a linear aggregation model, and every weak hypothesis is learnt with the whole set of learning data
- In contrast, decision tree is a conditional aggregation model and learns weak hypotheses by focusing on a subset of learning data. See more detail in next section

## [Decision Tree](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/209_handout.pdf)
### What
- A traditional model that realizes conditional aggregation
TODO: add image (aggregation summary)
### Why
- Mimic human's decision making process
- Good explainability
- Easy to implement (recursive tree model)
- Efficient in prediction and training
### How
- C&RT algorithm (Classification & Regression Tree)
TODO: add image
- Impurity function
TODO: add image
- Termination condition
TODO: add image
- Regularization by pruning
TODO: add image
- Branching on categorical feature
TODO: add image
- Missing feature by surrogate branching
TODO: add image
### When and Where
- Decision tree is not a practical model, but it is a core building block for **Random Forest** and **Gradient Boosting**
### Cons
- Heuristics with mostly little theoretical explanations
- Easy to overfit(fully grown tree usually has large variance) or underfit(not able to find a rule to split data properly)
- Solution is **Random Forest** or **Gradient Boosting** introduced in the next sections

## [Random Forest](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/210_handout.pdf)
### What
- Random forest == bagging + fully-grown C&RT decision tree on randomly projected subspace
### Why
- Decision tree (esp. fully-grown tree) has large variance, and bagging is able to reduce variance
- Inherit pros of C&RT
- Highly parallel/efficient to learn
### How
- bagging + fully-grown C&RT decision tree
TODO: add image
- Diversify by feature selection (feature expansion) when computing decision condition each time
    - feature selection: select some features from original ones (a special case of feature expansion)
    - feature expansion: use some random linear combination of original feature basis to form a projection matrix, and use it project original features onto random subspace
TODO: add image
- Use OOB (out-of-bag) examples to select G() (similar as validation)
    - about 1/e of examples are out-of-bag
    - no re-train needed, compared with validation
TODO: add image
- **Feature selection by importance**
    - In general, feature importance can be computed with feature permutation, i.e. if a random feature value does impact the classification/regression result, then it is an importance feature in this model
    TODO: add image
    - In random forest, the feature permutation is only performed within OOB examples
    TODO: add image
### When and Where
- Random forest is also a very popular model 
### Cons
- May need lots of trees if the whole random process is too unstable

## [Gradient Boosting Decision Tree](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)
### What
### Why
### How
### When and Where
### Cons

## [Summary](https://www.csie.ntu.edu.tw/~htlin/mooc/doc/211_handout.pdf)

